# Visual Question and Answering using LLM's

## Visual Question Answering using Gemini-1.5 flash model
The primary goal of VQA is to develop intelligent systems capable of interpreting image content and generating meaningful responses in natural language. I proposed a novel VQA system leveraging the Gemini-1.5 Flash Transformer model.

## Gemini 1.5 flash
Gemini 1.5 Flash is the newest addition to the Gemini family of large language models, and itâ€™s specifically designed to be fast, efficient, and cost-effective for high-volume tasks.
Gemini 1.5 Flash does parallel computation of attention and feedforward components. It is trained with higher-order preconditioned methods for improved quality.
Gemini 1.5 Flash while being smaller and way more efficient and faster to serve, maintains high levels of performance even as its context window increases.
The model achieves near-perfect recall on long-context retrieval tasks across modalities and improve the state-of-the-art in long-document QA, long-video QA and long-context ASR (Automatic Speech Recognition).


## To use this repository
fork the github repo using: ``` git clone https://github.com/Naveen23-code/VQA-using-LLM.git``` <br>
After successfully cloning the repository into a local directory follow the below steps to run the Application.
## To run Streamlit Application
use command: ``` streamlit run file_name.py ```
